####### MY PAPERS
@inproceedings{CDC,
	title        = {Decentralized constrained optimization: Double averaging and gradient projection},
	author       = {Shahriari-Mehr, Firooz and Bosch, David and Panahi, Ashkan},
	year         = 2021,
	booktitle    = {2021 60th IEEE Conference on Decision and Control (CDC)},
	pages        = {2400--2406},
	organization = {IEEE},
	abbr         = {DAGP v1<br>2021},
	abstract     = {In this paper, we consider the convex, finite-sum minimization problem with explicit convex constraints over strongly connected directed graphs. The constraint is an intersection of several convex sets each being known to only one node. To solve this problem, we propose a novel decentralized projected gradient scheme based on local averaging and prove its convergence using only local functionsâ€™ smoothness. Experimental studies demonstrate the effectiveness of the proposed method in both constrained and unconstrained problems.},
	code         = {https://github.com/Firooz-shahriari/Decentralized-Constrained-Optimization},
	html         = {https://ieeexplore.ieee.org/abstract/document/9683355}
}
@inproceedings{CMOD,
	title        = {New Dictionary Learning Methods for Two-Dimensional Signals},
	author       = {Shahriari-Mehr, Firooz and Parsa, Javad and Babaie-Zadeh, Massoud and Jutten, Christian},
	year         = 2020,
	booktitle    = {2020 28th European Signal Processing Conference (EUSIPCO)},
	organization = {IEEE},
	abbr         = {2D-CMOD<br>2020},
	abstract     = {By growing the size of signals in one-dimensional dictionary learning for sparse representation, memory consumption and complex computations restrict the learning procedure. In applications of sparse representation and dictionary learning in two-dimensional signals (e.g. in image processing), if one opts to convert two-dimensional signals to one-dimensional ones, and use the existing one-dimensional dictionary learning and sparse representation techniques, too huge signals and dictionaries will be encountered. Two-dimensional dictionary learning has been proposed to avoid this problem. In this paper, we propose two algorithms for two-dimensional dictionary learning. According to our simulations, the proposed algorithms have noticeable improvement in both convergence rate and computational load in comparison to one-dimensional methods.},
	code         = {https://github.com/Firooz-shahriari/Dictionary-Learning},
	html         = {https://ieeexplore.ieee.org/document/9287479}
}
@article{DAGP_Arxiv,
	title        = {Double Averaging and Gradient Projection: Convergence Guarantees for Decentralized Constrained Optimization},
	author       = {Shahriari-Mehr, Firooz and Panahi, Ashkan},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2210.03232},
	arxiv        = {2210.03232},
	abbr         = {DAGP v2<br>2023},
	abstract     = {We consider a generic decentralized constrained optimization problem over static, directed communication networks, where each agent has exclusive access to only one convex, differentiable, local objective term and one convex constraint set. For this setup, we propose a novel decentralized algorithm, called DAGP (Double Averaging and Gradient Projection), based on local gradients, projection onto local constraints, and local averaging. We achieve global optimality through a novel distributed tracking technique we call distributed null projection. Further, we show that DAGP can be used to solve unconstrained problems with non-differentiable objective terms with a problem reduction scheme. Assuming only smoothness of the objective terms, we study the convergence of DAGP and establish sub-linear rates of convergence in terms of feasibility, consensus, and optimality, with no extra assumption (e.g. strong convexity). For the analysis, we forego the difficulties of selecting Lyapunov functions by proposing a new methodology of convergence analysis in optimization problems, which we refer to as aggregate lower-bounding. To demonstrate the generality of this method, we also provide an alternative convergence proof for the standard gradient descent algorithm with smooth functions. Finally, we present numerical results demonstrating the effectiveness of our proposed method in both constrained and unconstrained problems. In particular, we propose a distributed scheme by DAGP for the optimal transport problem with superior performance and speed.},
	code         = {https://github.com/Firooz-shahriari/Decentralized-Constrained-Optimization}
}
@article{ASY-DAGP,
	title        = {Asynchronous Decentralized Optimization with Constraints: Achievable Speeds of Convergence for Directed Graphs},
	author       = {Shahriari-Mehr, Firooz and Panahi, Ashkan},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2401.03136},
	arxiv        = {2401.03136},
	abbr         = {ASY-DAGP<br>2024},
	abstract     = {We propose a novel decentralized convex optimization algorithm, called ASY-DAGP, where every agent has an individual objective function and constraint set. Agents compute at different speeds, and their communication may be delayed and directed. Employing local buffers, ASY-DAGP is communication-efficient and handles difficult scenarios such as message failure. We validate these facts by numerical experiments. Further, by analyzing ASY-DAGP, we provide first sublinear guarantees on rates of convergence in the above setup with mild assumptions. For this purpose, we introduce a novel analysis approach, called linear quadratic (LQ-) PEP, tied to the celebrated PEP framework. Our method does not need the design of Lyapunov functions and instead provides a novel insight into the optimization algorithms from the perspective of linear systems.},
	code         = {https://github.com/Firooz-shahriari/Asynchronous-Decentralized-Constrained-Optimization}
}
