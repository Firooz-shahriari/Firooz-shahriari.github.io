---
layout: page
title: ParsBERT
description: A monolingual language model based on Googleâ€™s BERT architecture.
img: assets/img/prj-parsbert-01.png
importance: 2
category: research
related_publications: parsBERT
---

# ðŸ¤— ParsBERT: Transformer-based Model for Persian Language Understanding


ParsBERT trained on a massive amount of public corpora ([Persian Wikidumps](https://dumps.wikimedia.org/fawiki/), [MirasText](https://github.com/miras-tech/MirasText)) and six other manually crawled text data from a various type of websites ([BigBang Page](https://bigbangpage.com/) `scientific`, [Chetor](https://www.chetor.com/) `lifestyle`, [Eligasht](https://www.eligasht.com/Blog/) `itinerary`,  [Digikala](https://www.digikala.com/mag/) `digital magazine`, [Ted Talks](https://www.ted.com/talks) `general conversational`, Books `novels, storybooks, short stories from old to the contemporary era`).

As a part of ParsBERT methodology, an extensive pre-processing combining POS tagging and WordPiece segmentation was carried out to bring the corpora into a proper format. 

Follow the rest of [the repo](https://github.com/hooshvare/parsbert) for more details.

Paper link: [10.1007/s11063-021-10528-4](https://doi.org/10.1007/s11063-021-10528-4)

<p align="center">
    <a href="https://www.youtube.com/watch?v=Fyirkq668PE"><img src="/assets/img/prj-parsbert-02.png" width="800"></a>
    <br>
    <em>YouTube Demo !</em>
</p>

